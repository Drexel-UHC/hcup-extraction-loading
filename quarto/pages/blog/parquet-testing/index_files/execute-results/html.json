{
  "hash": "a6682da96fa7907d15ee0a011d20adf1",
  "result": {
    "markdown": "---\ntitle: \"Parquet Benchmarking!\"\nsubtitle: \"Lets do some benchmark some CSV vs Parquet tests.\"\nauthor: \"Ran Li\"\ndate: \"2023-02-02\"\nformat: \n  html:\n    df-print: kable\neditor_options: \n  chunk_output_type: console\nexecute: \n  warning: false\n---\n\n\n## Intro\n\nSimilar to a CSV file, Parquet is a type of file. The difference is that Parquet is designed as a columnar storage format to support complex data processing.\n\n- Apache Parquet is column-oriented and designed to bring efficient columnar storage (blocks, row group, column chunks…) of data compared to row-based like CSV\n- Parquet files were designed with complex nested data structures in mind.\n- Apache Parquet is built to support very efficient compression and encoding schemes (see Google Snappy)\n- Apache Parquet allows lower storage costs for data files and **maximizes the effectiveness of querying data with serverless OLAP technologies** like DuckDB, Amazon Athena, Redshift Spectrum, BigQuery, and Azure Data Lakes.\n\nThe “big data” association with this format may give the impression that the format is limited to specific use cases. However, as the format has moved out of the shadow of complex Hadoop big data solutions, it has gained broader support. For example, teams build low-cost, high-performance serverless business intelligence stacks with Apache Parquet, Tableau, and Amazon Athena.\n\n## Columnar Storage Test\n\nLet generate two tables both 2 trillion rows long. With a single column called pet with two possible values `cat` or `dog`. The first table we call `df_ordered` has 1 trillion cats and 1 trillion dogs ordered. The second table called `df_unordered` has the same values but alternating cat dog cat dog. \n\nLets see the parquet storage sizes!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(arrow)\nlibrary(data.table)\nlibrary(stringr)\n\n## Generate tables \nn = 10^7\ndf_ordered = tibble(pet = c(rep(\"cat\",n),rep(\"dog\",n)))\ndf_unordered = tibble(pet = rep(c(\"cat\",\"dog\"),n))\n\n## Write to parquet and CSV\ndf_ordered %>% fwrite('df_ordered.csv')\ndf_ordered %>% write_parquet(sink = 'df_ordered.parquet')\ndf_unordered %>% fwrite('df_unordered.csv')\ndf_unordered %>% write_parquet(sink = 'df_unordered.parquet')\n\n## Get file sizes\ntibble( file = list.files(pattern = \"df_\")) %>% \n  mutate(size_bytes = file.size(file),\n         size_kb = size_bytes/1024 %>% round(1),\n         size_mb = size_kb/1024 %>% round( 0))  %>% \n  # mutate(file = ifelse(str_detect(file, 'unordered'),\n  #                      \"Unordered: cat, dog, cat, dog\",\n  #                      \"Ordered: cat, cat, dog, dog\")) %>% \n  select(file, size_mb) %>% \n  arrange(size_mb)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|file                 |    size_mb|\n|:--------------------|----------:|\n|df_ordered.parquet   |  0.0006790|\n|df_unordered.parquet |  0.1145401|\n|df_ordered.csv       | 95.3674364|\n|df_unordered.csv     | 95.3674364|\n\n</div>\n:::\n:::\n\n\n\nIn CSV both the ordered and unordered files are ~ 95 megabytes. As expected in parquet we see massive storage benefits. Columnar storage in a Parquet file saves on file size by compressing and encoding similar data values in the same column together, rather than spreading them across rows. This is because columnar storage is optimized for reading and processing specific columns, rather than entire rows.\n\nIn the example you provided, a columnar storage format like Parquet would efficiently store the values \"cat\" and \"dog\" by encoding them once and referencing their encoding for each instance in the column, rather than repeating the entire value for each row. This results in a more compact representation of the data and reduces the overall file size.\n\n## Parquet Format vs. CSV \n\nCSV is ubiquitous and straightforward. Many tools like Excel, Google Sheets, and a host of others can generate CSV files. You can even create CSV files with your favorite text editing tool.\n\nTHe price you pay? Do CSV files have a cost?\n\n- columnar databases (column oriented OLAP like DuckDB, AWS Athena, Bigquery, AWS Redshift) charge by amount of data scanned. \n  - CSV files have to be completely scanned to run a query.\n  - Parquet files only selected columns are read.\n- Storage size is much lower, so lower storage costs.\n\n##",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}