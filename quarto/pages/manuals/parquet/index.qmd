---
title: "Parquet"
author: "Ran Li"
categories: 
  - "big data"
  - "storage"
  - "parquet"
execute: 
  warning: false
editor_options: 
  chunk_output_type: console
---

![](images/parquet-logo.png){alt="Parquet Logo" fig-alt="Parquet Logo" fig-align="center" width="287"}

Apache Parquet is a popular column storage file format common used to efficiently store large datasets and has the `.parquet` extension. Key features of parquet are:

-   language independent

-   recognized file format used by many systems

-   stores data in column layout

-   stores metadata

The later two points make it alternative to CSV in that parquet workflows have efficient storage (much smaller file sizes than CSV), accessible metadata for columns and efficient queries.

This blog will use a few packages.Lets load them first

```{r}
## Dependencies
library(arrow) # Working with parquet
library(purrr) # Functional loops
library(dplyr) # Data manipulation
library(reactable) # Nice tables
```

First lets put together some data to play around with. We'll start with the Palmer's penguins data then copy it a thousand times to make a rather large table.

```{r}
# Use the penguins data set
data(penguins, package = "palmerpenguins")
big_penguins = 1:1000 %>% map_df(~return(penguins))
glimpse(big_penguins)
```

A glimpse of the data shows it has 344,000 rows with 8 columns.

Below we conduct some CSV vs Parquet experiments

## File Sizes

Lets do our first experiemnt. If we were to store this piece of data somewhere how large would it be.

```{r}

# Create a temporary file for the output
write_parquet(big_penguins, sink = 'processed/test.parquet')
write_csv_arrow(big_penguins, sink = 'processed/test.csv')

# get file sizes
tibble(format = c("csv","parquet"),
       size_bytes = c(file.size('processed/test.csv'),
                      file.size('processed/test.parquet'))) %>% 
  reactable()

```

We can see the parquet format is only 88 KB in size compared to the 16,760 KB CSV; in this example CSV storage take up **`r round(file.size('processed/test.csv')/file.size('processed/test.parquet'),0)`** more space compared to parquet. If we were storing things on the cloud, in this case, parquet format would be almost 200 times cheaper!

## Queries

Parquets are the defaco standard for dat
