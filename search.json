[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nParquet Benchmarking!\n\n\nLets do some benchmark some CSV vs Parquet tests.\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nRan Li\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatawarehouse Proposal 2022\n\n\nRequest from Alina to draft 2 paragraph infrastuctre proposal for grant\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nRan Li\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatawarehouse Proposal 2022 Slides\n\n\nBackground slides summarizing some basic data warehousing concepts.\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nRan Li\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HCUP Resource at the UHC",
    "section": "",
    "text": "The Healthcare Cost and Utilization Project (HCUP) includes the largest collection of longitudinal hospital care data in the United States."
  },
  {
    "objectID": "index.html#databases",
    "href": "index.html#databases",
    "title": "HCUP Resource at the UHC",
    "section": "Databases",
    "text": "Databases\nHCUP has both nationwide HCUP and state-specific HCUP databases.\n\nNationwide\n\nNIS - National Inpatient Sample\nKID - Kids’ Inpatient Database\nNASS Nationwide Ambulatory Surgery Sample\nNEDS - Nationwide Emergency Department Sample\nNRD - Nationwide Readmissions Database\n\nState-specific\n\nSID - State Inpatient Databases\nSASD - State Ambulatory Surgery and Services Databases\nSEDD - State Emergency Department Databases"
  },
  {
    "objectID": "index.html#what-is-hcup",
    "href": "index.html#what-is-hcup",
    "title": "HCUP Resource at the UHC",
    "section": "What is HCUP",
    "text": "What is HCUP\n\nThe Healthcare Cost and Utilization Project (HCUP) includes the largest collection of longitudinal hospital care data in the United States."
  },
  {
    "objectID": "pages/blog/datawarehouse-proposal-2022/index.html",
    "href": "pages/blog/datawarehouse-proposal-2022/index.html",
    "title": "Datawarehouse Proposal 2022",
    "section": "",
    "text": "Data warehousing is a critical component of any research or data organization; it consists of data storage, a database engine, and a way to organize database assets. Properly implemented, a data warehouse will reduce infrastructure/staffing costs, reduce data bottlenecks, and increase research synthesis. Therefore, academic organizations need to keep abreast of industry trends and best practices to get the most they can out of their data assets.\nHere we discuss a few modern data warehousing solutions that we will/have implemented at our organization. Firstly, data is stored in a columnar format such as .parquet which has been optimized for efficient storage and rapid analysis of large amounts of data. Secondly, the intentional utilization of a multilingual and lightweight database engine (e.g. Apache Arrow or DuckDB) that is designed for analytical tasks rather than transactional purposes. Lastly, we orchestrate the data storage and analytics within the context of best-practices frameworks such as Targets (R framework from pharma) or DBT (SQL framework from the health analytics sector). Taken together these tools compose a modern data stack that will not only enable us to effectively utilize big data to rapidly deliver on proposed research goals but also synergize with the staffing and costs parameters within our organization."
  },
  {
    "objectID": "pages/blog/infrastructure-exploration-slides/index.html",
    "href": "pages/blog/infrastructure-exploration-slides/index.html",
    "title": "Datawarehouse Proposal 2022 Slides",
    "section": "",
    "text": "Some slides"
  },
  {
    "objectID": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#overview",
    "href": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#overview",
    "title": "UHC - HCUP Resource",
    "section": "Overview",
    "text": "Overview\n\nDatabase\n\nDefine database and evaluate databases\nDatabase recommendations\n\nDatawarehouse\n\nDefine data warehouse and evaluate two options\nDatawarehouse recommendations\n\nDemo of current prototype\n\ntargets pipeline on the backend\ndbt-like documentation on the front-end"
  },
  {
    "objectID": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#database",
    "href": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#database",
    "title": "UHC - HCUP Resource",
    "section": "Database",
    "text": "Database\n\nDatabase is a collection of data\nDatabase Management System (DBMS) is a software used to manage databases.\nDBMS can be evaluated based on two criteria\n\nPurpose: Transactional vs Analytical\nInfrastructure: Sever vs Serverless"
  },
  {
    "objectID": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#database-purpose",
    "href": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#database-purpose",
    "title": "UHC - HCUP Resource",
    "section": "Database Purpose",
    "text": "Database Purpose"
  },
  {
    "objectID": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#database-architecture",
    "href": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#database-architecture",
    "title": "UHC - HCUP Resource",
    "section": "Database Architecture",
    "text": "Database Architecture"
  },
  {
    "objectID": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#comparisons",
    "href": "pages/blog/infrastructure-exploration-slides/slides/hcup-infrastructure-proposal-slides.html#comparisons",
    "title": "UHC - HCUP Resource",
    "section": "Comparisons",
    "text": "Comparisons\ndb engine graph\nTakeaway:\n\nBig Query is analytical, sever based DBMS\nPostgreSQL: is a transactional, server based DMBS\nDuckDB/Arrow: analytical, server-less DBMS\n\nRecent venture investment indicates ong term manintainblity\nLight-weight\nMulti-lingual\nNo cost"
  },
  {
    "objectID": "pages/blog/parquet-testing/index.html#intro",
    "href": "pages/blog/parquet-testing/index.html#intro",
    "title": "Parquet Benchmarking!",
    "section": "Intro",
    "text": "Intro\nSimilar to a CSV file, Parquet is a type of file. The difference is that Parquet is designed as a columnar storage format to support complex data processing.\n\nApache Parquet is column-oriented and designed to bring efficient columnar storage (blocks, row group, column chunks…) of data compared to row-based like CSV\nParquet files were designed with complex nested data structures in mind.\nApache Parquet is built to support very efficient compression and encoding schemes (see Google Snappy)\nApache Parquet allows lower storage costs for data files and maximizes the effectiveness of querying data with serverless OLAP technologies like DuckDB, Amazon Athena, Redshift Spectrum, BigQuery, and Azure Data Lakes.\n\nThe “big data” association with this format may give the impression that the format is limited to specific use cases. However, as the format has moved out of the shadow of complex Hadoop big data solutions, it has gained broader support. For example, teams build low-cost, high-performance serverless business intelligence stacks with Apache Parquet, Tableau, and Amazon Athena."
  },
  {
    "objectID": "pages/blog/parquet-testing/index.html#columnar-storage-test",
    "href": "pages/blog/parquet-testing/index.html#columnar-storage-test",
    "title": "Parquet Benchmarking!",
    "section": "Columnar Storage Test",
    "text": "Columnar Storage Test\nColumnar storage in a Parquet file saves on file size by compressing and encoding similar data values in the same column together, rather than spreading them across rows. This is because columnar storage is optimized for reading and processing specific columns, rather than entire rows.\nIn the example you provided, a columnar storage format like Parquet would efficiently store the values “cat” and “dog” by encoding them once and referencing their encoding for each instance in the column, rather than repeating the entire value for each row. This results in a more compact representation of the data and reduces the overall file size.\nLet generate two tables both 2 million rows long. With a single column called pet with two possible values cat or dog. The first table we call df_ordered has 1 million cats and 1 million dogs ordered. The second table called df_unordered has the same values but alternating cat dog cat dog.\nLets see the parquet storage sizes!\n\n## Generate tables \nn = 10^6\ndf_ordered = tibble(pet = c(rep(\"cat\",n),rep(\"dog\",n)))\ndf_unordered = tibble(pet = rep(c(\"cat\",\"dog\"),n))\n\n## Write to parquet and CSV\ndf_ordered %>% fwrite('df_ordered.csv')\ndf_ordered %>% write_parquet(sink = 'df_ordered.parquet')\ndf_unordered %>% fwrite('df_unordered.csv')\ndf_unordered %>% write_parquet(sink = 'df_unordered.parquet')\n\n\ntibble( file = list.files(pattern = \"df_\")) %>% \n  mutate(size_bytes = file.size(file),\n         size_kb = size_bytes/1024 %>% round(1),\n         size_mb = size_kb/1024 %>% round( 0))  %>% \n  select(file, size_mb) %>% \n  arrange(size_mb) %>% \n  filter(!str_detect(file,'long_str'))\n\n\n\n\n\nfile\nsize_mb\n\n\n\n\ndf_ordered.parquet\n0.0004826\n\n\ndf_unordered.parquet\n0.0118704\n\n\ndf_ordered.csv\n9.5367479\n\n\ndf_unordered.csv\n9.5367479\n\n\n\n\n\n\nAs expect massive win in terms of storage size for parquet. Even in the case of completely alternating cat, dog, cat, dog values in df_unordered.parquet we still see huge size reduction. This is because parquet will encode cat and dog in the metadata then in the data itself just store encoding (a much more memory efficient object than a string).\nSo hypothetically if we had much longer strings. The df_unordred should be similar in size because of this encoding.\n\ndf_unordered_long_str = df_unordered %>% \n  mutate(pet = paste0(pet,\".................\"))\n\ndf_unordered_long_str %>% write_parquet(sink = 'df_unordered_long_str.parquet')\ndf_unordered_long_str %>% fwrite('df_unordered_long_str.csv')\n\n\ntibble( file = list.files(pattern = \"df_\")) %>% \n  mutate(size_bytes = file.size(file),\n         size_kb = size_bytes/1024 %>% round(1),\n         size_mb = size_kb/1024 %>% round( 0))  %>% \n  select(file, size_mb) %>% \n  arrange(size_mb) %>% \n  filter(file%in%c('df_unordered.parquet','df_unordered_long_str.parquet'))\n\n\n\n\n\nfile\nsize_mb\n\n\n\n\ndf_unordered.parquet\n0.0118704\n\n\ndf_unordered_long_str.parquet\n0.0119753\n\n\n\n\n\n\nIndeed this encoding shows how even after appending a bunch of characteres to the pet column. We don’t see any increase in the parquet file size.\nFor giggles lets see what happens to CSV size when we add characters to the pet column values.\n\ntibble( file = list.files(pattern = \"df_\")) %>% \n  mutate(size_bytes = file.size(file),\n         size_kb = size_bytes/1024 %>% round(1),\n         size_mb = size_kb/1024 %>% round( 0))  %>% \n  select(file, size_mb) %>% \n  arrange(size_mb) %>% \n  filter(file%in%c('df_unordered.csv','df_unordered_long_str.csv'))\n\n\n\n\n\nfile\nsize_mb\n\n\n\n\ndf_unordered.csv\n9.536748\n\n\ndf_unordered_long_str.csv\n41.961675\n\n\n\n\n\n\nAs expected we see an increase in size due to each cell value changing. Parquet smartly avoids this by using encoding!"
  },
  {
    "objectID": "pages/blog/parquet-testing/index.html#cons-of-csv-and-parquet",
    "href": "pages/blog/parquet-testing/index.html#cons-of-csv-and-parquet",
    "title": "Parquet Benchmarking!",
    "section": "Cons of CSV and parquet",
    "text": "Cons of CSV and parquet\nCSV is ubiquitous and straightforward. Many tools like Excel, Google Sheets, and a host of others can generate CSV files. You can even create CSV files with your favorite text editing tool.\nThe price you pay for CSV files?\n\ncolumnar databases (column oriented OLAP like DuckDB, AWS Athena, Bigquery, AWS Redshift) charge by amount of data scanned.\n\nCSV files have to be completely scanned to run a query.\nParquet files only selected columns are read.\n\nStorage size is much lower, so lower storage costs.\nno self contained metadata\n\nParquet on the other handle resolves a lot of these issues but is less accessible. You can’t just open it in excel (currently). Hopefully as parquet becomes more prevalent, I hope mainstream tools such as MS excel will be able to use parquet."
  },
  {
    "objectID": "pages/blog/parquet-testing/index.html#parquet-arrow-empowers-existing-data-science-workflows",
    "href": "pages/blog/parquet-testing/index.html#parquet-arrow-empowers-existing-data-science-workflows",
    "title": "Parquet Benchmarking!",
    "section": "Parquet + Arrow: empowers existing data science workflows",
    "text": "Parquet + Arrow: empowers existing data science workflows\nto polish…\nthe syneregy between parquet + embedded databases (e.g. Arrow) brings so much power to existing R/Python workflows. Most of the time enables you do big data without any inffrastructure or tool change"
  },
  {
    "objectID": "pages/blog/parquet-testing/index.html#parquet-and-the-modern-data-stack",
    "href": "pages/blog/parquet-testing/index.html#parquet-and-the-modern-data-stack",
    "title": "Parquet Benchmarking!",
    "section": "Parquet and the modern data stack",
    "text": "Parquet and the modern data stack\nto polish…\nStorage is cheap, computation is expensive. Was how things use to be.\nNow modern databases are shifting towards serverless OLAP engines (DuckDB, AWS Athena, Google BigQuery) which charge based on consumption (e.g. how many megabytes of data scanned)…\nThis cost mechanism when paired the benefits of columnar storage such as parquet means now computation/queries are extremely cheap.\nSo the modern datastack is often some combination of the two."
  },
  {
    "objectID": "pages/manuals/parquet/index.html",
    "href": "pages/manuals/parquet/index.html",
    "title": "Parquet",
    "section": "",
    "text": "Apache Parquet is a popular column storage file format common used to efficiently store large datasets and has the .parquet extension. Key features of parquet are:\nThe later two points make it alternative to CSV in that parquet workflows have efficient storage (much smaller file sizes than CSV), accessible metadata for columns and efficient queries.\nThis blog will use a few packages.Lets load them first\nFirst lets put together some data to play around with. We’ll start with the Palmer’s penguins data then copy it a thousand times to make a rather large table.\nA glimpse of the data shows it has 344,000 rows with 8 columns.\nBelow we conduct some CSV vs Parquet experiments"
  },
  {
    "objectID": "pages/manuals/parquet/index.html#file-sizes",
    "href": "pages/manuals/parquet/index.html#file-sizes",
    "title": "Parquet",
    "section": "File Sizes",
    "text": "File Sizes\nLets do our first experiemnt. If we were to store this piece of data somewhere how large would it be.\n\n# Create a temporary file for the output\nwrite_parquet(big_penguins, sink = 'processed/test.parquet')\nwrite_csv_arrow(big_penguins, sink = 'processed/test.csv')\n\n# get file sizes\ntibble(format = c(\"csv\",\"parquet\"),\n       size_bytes = c(file.size('processed/test.csv'),\n                      file.size('processed/test.parquet'))) %>% \n  reactable()\n\n\n\n\n\n\nWe can see the parquet format is only 88 KB in size compared to the 16,760 KB CSV; in this example CSV storage take up 192 more space compared to parquet. If we were storing things on the cloud, in this case, parquet format would be almost 200 times cheaper!"
  },
  {
    "objectID": "pages/manuals/parquet/index.html#queries",
    "href": "pages/manuals/parquet/index.html#queries",
    "title": "Parquet",
    "section": "Queries",
    "text": "Queries\nParquets are the defaco standard for dat"
  }
]